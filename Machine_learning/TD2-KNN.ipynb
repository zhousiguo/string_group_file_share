{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K nearest neighbors and cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical part : Non-consistency or NN for fixed k\n",
    "\n",
    "As said in the title, we are going to show that the 1-NN classifier is not consistent. We consider the risk $\\mathcal{R}(f) = \\mathbb{E}[\\ell(f(X), Y)]$.\n",
    "\n",
    "**1 ) Recall the definition of the consistency of a classifier $\\widehat{f}_n = \\mathcal{A}(D_n)$ where $\\widehat{f}_n: \\mathbb{R}^d \\longrightarrow \\lbrace 0,1 \\rbrace$.**\n",
    "\n",
    "**Set-up of the exercise** : We consider binary classification with the $0-1$ loss where we has an $n$-sample $D_n=(x_i,y_i)$ with $\\mathcal{X}=[0,1], \\mathcal{Y}=\\lbrace 0,1 \\rbrace$. \n",
    "\n",
    "This $n$-sample is sampled i.i.d. as follows:\n",
    " $x_i$ come from some distribution $P$ that has a density $p$ with respect to the Lebesgue measure on $\\mathcal{X}$. Labels $y_i$ are distributed so that $\\forall x \\in \\mathcal{X},\\eta(x)=\\mathbb{P}(Y=1\\vert X=x)=\\alpha > \\frac{1}{2}$.\n",
    "\n",
    "From this $n$-sample, we build a classifier $\\widehat{f}_n = \\mathcal{A}(D_n)$. Keep in mind that $\\widehat{f}_n$ is random due to $D_n = (X_1, \\ldots, X_n, Y_1, \\ldots, Y_n)$. Studying the consistency of the classification rule defined by $\\widehat{f}_n$ is equivalent to studying the behavior $\\widehat{f}_n$ when $n$ tends to infinity.\n",
    "\n",
    "\n",
    "**2) For the learning problem above, give an expression of Bayes predictor and the associated Bayes risk.**\n",
    "Bayes predictor $f^*(x)=\\Theta(\\mathbb{P}(Y=1|X=x)-\\frac{1}{2})$**\n",
    "Bayes risk $R^*=$\n",
    "\n",
    "**3) We start by considering a classifier $f$ that does not depend on the training data $D_n$.  Let $X$ be a testing random variable independent of $D_n$. Show that the risk of $f$ can be expressed as (note that any binary classifier can be written as $f(X)=\\mathbb{1}_{\\lbrace f(X)=1 \\rbrace}$): \n",
    "$$\\mathcal{R}(f)=\\alpha-(2 \\alpha -1)\\mathbb{E}_{X}[f(X)].$$**\n",
    "$\\mathcal{R}(f)=\\mathbb{E}_{X}[l(Y,f(X))]=\\mathbb{P}(l(Y,\\mathbb{1}_{f(X)=1})=1)=\\mathbb{P}(Y=0|f(X)=1)\\mathbb{P}(f(X)=1)+\\mathbb{P}(Y=1|f(X)\\neq1)(1-\\mathbb{P}(f(X=1))=(\\mathbb{P}(Y=0|f(X)=1)-\\mathbb{P}(Y=1|f(X)\\neq1))\\mathbb{P}(f(X)=1)+\\mathbb{P}(Y=1|f(X)\\neq1)=\\alpha+(1-2\\alpha)\\mathbb{P}(f(X)=1)=\\alpha-(2 \\alpha -1)\\mathbb{E}_{X}[f(X)]$\n",
    "\n",
    "**4) We now consider the classifier obtained from 1-NN $\\widehat{f}^1$ and we will show it is not consistent.**\n",
    "\n",
    " **a) Show that each $Y_i$ is independent from $(X_1,\\ldots, X_n)$ (a simple intuitive justification suffices).**\n",
    "\n",
    "**b) By introducing the variable $B_i(X)$ that is equal to 1 if the example $i$ from the sample is the nearest neighbor of $X$ and 0 otherwise, express $\\widehat{f}^1(X)$ as a sum of random variables .**\n",
    " $\\widehat{f}^1(X)=\\sum_{i=1}^n{B_i(X)\\widehat{f}^1(X_i)}$\n",
    "\n",
    "*Remark : it is natural to wonder how to do in practice when a test point is equidistant to several training points. In this exercise, since input variables have a density, this occurs with zero probability. In general, when this occurs, several strategies can be used to alleviate the problem, e.g., random allocation or allocation to the smallest $i$.*\n",
    "\n",
    "\n",
    "**c) What is the expectation of $Y_i$ given $(X_1,\\ldots, X_n)$? Give the expression of the expectation of $\\widehat{f}^1(x)$ given $(X_1, \\ldots X_n)$ (the expectation is taken with respect to $X$ and the $Y_i$'s).**\n",
    "\n",
    "**5) Compute the risk of $\\widehat{f}^1$ and deduce that the $1$-NN method is not consistent.**\n",
    "\n",
    "**6) We now consider the same binary classification problem on $\\mathcal{X}$ with the classifier $\\widehat{f}^K$ with $K$ nearest neighbors. To make our lives easier, we assume that $K$ is odd.**\n",
    "\n",
    "**a) Taking inspiration from the case $K=1$ above, show that the risk of the  classifier $\\widehat{f}^K$, conditioned on the training input data $(X_1, \\ldots X_n)$ (that is, expectation of the risk of $\\widehat{f}^K$ only with respect to the $Y_i$'s) can be expressed as a function of the probability that a binomial variable $U$ with parameters $K$ and $\\alpha$ is greater than $\\frac{K}{2}$ (We recall that $U$ is such a binomial random variable if $U$ can be written as the sum  $K$ independent Bernoulli random variables with parameter $\\alpha$).**\n",
    "\n",
    "**b) Show that in this case, the expectation of the risk of the $K$-NN classifier is stricly larger than the Bayes risk $1-\\alpha$. We can use the fact that the expectation of $\\widehat{f}^K$ is strictly less than one.**\n",
    "\n",
    "\n",
    "\n",
    "**\"Morale de l'histoire\"**\n",
    "\n",
    "The number of nearest neighbors has to depend on $n$. One can check that if we consider a sequence of integers $k_n$ such that $\\lim_{n\\rightarrow \\infty} k_n = \\infty$ and $\\lim_{n\\rightarrow \\infty} k_n/n = 0$,then the assumptions of Stone's theorem are satisfied for all distributions. Thus, with this sequence $k_n$, we get universal consistency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation : K-nearest neighbors and cross-validation\n",
    "\n",
    "For this practical session, we will work on the real data mnist_digits.mat (digits), that can be downloaded from the course web page.\n",
    "\n",
    "For classification problems with $K$ classes, we call the \"confusion matrix\" associated to data $D_n=(x_t,y_t)$ the matrix $M \\in \\mathbb{N}^{K \\times K}$ such that $M_{i,j}$ is the number of elements with true class $i$ and predicted class $j$.\n",
    "\n",
    "**NB**: Given that there are more than $66000$ images in the dataset, we only work on a subset of these $66000$ images so as to not go beyond the memory of your computer.\n",
    "\n",
    "**1) Start by getting acquainted with the data. They are composed of a vector of labels `y` and images of size 28x28, given in matrix `x` of linearized vectors (each line of the matrix `x` corresponds to a single image).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import plotly\n",
    "import numpy as np\n",
    "#import plotly.plotly as py\n",
    "#import plotly.graph_objs as go\n",
    "#plotly.offline.init_notebook_mode()\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "from scipy import stats\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a1da8dd90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Chargement des donn√©es\n",
    "data = sio.loadmat('mnist_digits.mat')\n",
    "X_total = np.array(data['x'])\n",
    "Y_total = np.array(data['y'])\n",
    "n_total = X_total.shape[0]\n",
    "\n",
    "# Affichage d'une image\n",
    "ind_im = 2000\n",
    "im = X_total[ind_im,:].reshape((28, 28))\n",
    "\n",
    "fig, (ax1) = plt.subplots(nrows=1, figsize=(8,8))\n",
    "ax1.set_title('Image nombre MNIST')\n",
    "ax1.imshow(im, extent=[0,100,0,1], aspect=100, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Select $6000$ images at random in the dataset.** \n",
    "\n",
    "**c) Split the images into two parts (with proportions $1/3,2/3$ for example) : a training set and a testing set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b75f79cbfcf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mX_rdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_total\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_rdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#X_tot=X_total\n",
    "X_rdm=[]\n",
    "Y_rdm=[]\n",
    "for i in range (6000):\n",
    "    n=np.random.randint(len(X_total))\n",
    "    X_rdm.append(X_total[n])\n",
    "    Y_rdm.append(Y_total[n])\n",
    "    np.delete(X_total,n)\n",
    "print(len(X_rdm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) We will now implement the nearest neighbor classification rule. For this, you may use the function `cdist` from the module `scipy.spatial.distance`. This function allows, given two design matrices to compute all squared Euclidean distances between points.**\n",
    "\n",
    "**a) Write a function that takes as inputs the number $K$ of desired nearest neighbors, the training data, the testing data, and outputs the confusion matrix on the test data.**\n",
    "\n",
    "For the confusion matrix, you can use the commande `confusion_matrix` from the module `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Display the classification error on the training and testing data as a function of $k$ (number of nearest neighbors) (note that the implicit complexity / smoothness of the learned function is decreasing with the number of nearest neighbors). You can vary $k$ from 1 to 20 for example.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Split your training data into a reduced training data and a validation set (this technique is called simple validation). By using the previous code, write a function that will select the best $K$ by using the validation set. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) Split several times randomly the original training data. Is the estimator of $K$ stable?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) We now wish to select $K$ by cross-validation. Implement $V$-fold cross-validation for $V=8$ and select the best $K$. Vary randomly the split of the data into the folds and look at the behavior of the selected $K$. What do you notice?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
